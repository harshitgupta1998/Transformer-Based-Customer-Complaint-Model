# -*- coding: utf-8 -*-
"""Transformer-Customer_Complaint_Model

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1k96vNMf4XgUmJNeEsDpHq6CzzmSku85o
"""

# Import the library required
!pip -q install accelerate -U
!pip -q install transformers[torch]
!pip -q install datasets

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
from transformers import pipeline

"""# Hugging Face models - Pipeline()

#### Using pipeline model as base
"""

from transformers import pipeline

sentiment_model = pipeline(task="sentiment-analysis",
                         model="cardiffnlp/twitter-roberta-base-sentiment-latest",
                           device="cuda")

sentiment_model("Over heating issue don't by this product camera was good")

sentiment_model("Waste of money")

sentiment_model("Nice product under 24k .... overall good")

# Import review dataset to review each row
import pandas as pd
user_review_data=pd.read_csv("https://raw.githubusercontent.com/harshitgupta1998/TransformerModel-Sentiment/refs/heads/main/data/Review_Data.csv")
user_review_data=user_review_data.sample(50)
user_review_data["Review"]

# Apply the model to predict each row
user_review_data["Predicted_Sentiment"] = user_review_data["Review"].apply(lambda x: sentiment_model(x)[0]["label"])
user_review_data

#Clear the cache in GPU
import torch
torch.cuda.empty_cache()

# Sentiment Analysis without pipeline to provide more functionality
from transformers import AutoTokenizer, AutoModelForSequenceClassification

tokenizer = AutoTokenizer.from_pretrained("cardiffnlp/twitter-roberta-base-sentiment")

model = AutoModelForSequenceClassification.from_pretrained("cardiffnlp/twitter-roberta-base-sentiment")

import numpy as np
raw_text = "This is a great book"
encoded_input = tokenizer(raw_text, return_tensors='pt')
output = model(**encoded_input)
logits = output.logits.detach().numpy()
y_pred = np.argmax(logits)
y_pred

#Code for passing multiple examples to the above model

import numpy as np
# Prepare the input texts
texts = [
    "This is a great book",
    "The food was not tasty and it was very cold",
    "The weather is very good today",
]

# Tokenize and encode the input texts
encoded_inputs = tokenizer(texts, padding=True, return_tensors="pt")

# Pass the encoded inputs to the model
outputs = model(**encoded_inputs)

# Get the model's predictions
logits = outputs.logits.detach().cpu().numpy()
print(logits)
# Find the predicted class for each input
predictions = np.argmax(logits, axis=1)
predictions_text={0:'Neutral',1:'Negative',2:'Positive'}
# Print the predictions
for i in predictions:
  print(predictions_text[i])

"""#### Finetuning HuggingFace model to apply on customer complaints

##### Bank Complaints Data
"""

!wget https://github.com/venkatareddykonasani/Datasets/raw/master/Bank_Customer_Complaints/complaints_v2.zip
!unzip -o complaints_v2.zip
complaints_data = pd.read_csv("/content/complaints_v2.csv")
complaints_data.head()

#Use distilbert model for sentimental analysis without fine tuning
from transformers import pipeline
distilbert_model = pipeline(task="text-classification",
                            model="distilbert-base-uncased",
                            )

sample_data=complaints_data.sample(100, random_state=42)
sample_data["text"]=sample_data["text"].apply(lambda x: " ".join(x.split()[:350]))
sample_data["bert_predicted"] = sample_data["text"].apply(lambda x: distilbert_model(x)[0]["label"])
#Default prediction is not a number LABEL_1, LABEL_0
sample_data["bert_predicted_num"]=sample_data["bert_predicted"].apply(lambda x: x[-1])
sample_data["bert_predicted_num"] = sample_data["bert_predicted_num"].astype(int)
sample_data.head()

# Accuracy of the model without fine-tuning
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(sample_data["label"], sample_data["bert_predicted_num"])
print(cm)
accuracy=cm.diagonal().sum()/cm.sum()
print(accuracy)

!pip -q install accelerate -U
!pip -q install transformers[torch]
!pip -q install datasets

##Finetuning the model with our data
from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments
from transformers import Trainer, TrainingArguments
from datasets import load_dataset, DatasetDict, ClassLabel, Dataset
import pandas as pd
from sklearn.model_selection import train_test_split
import torch

#The target variable must be named as "label" - Verify it, before proceeding
print(sample_data.columns)

Sample_data = Dataset.from_pandas(sample_data)
# Split the dataset into training and testing sets
train_test_split = Sample_data.train_test_split(test_size=0.2)  # 80% training, 20% testing
dataset = DatasetDict({
    'train': train_test_split['train'],
    'test': train_test_split['test']
})
dataset

"""### Load the tokenizer"""

# Load the tokenizer
tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')

# Padding
tokenizer.pad_token = tokenizer.eos_token
tokenizer.pad_token_id = tokenizer.eos_token_id
tokenizer.add_special_tokens({'pad_token': '[PAD]'} )

def tokenize_function(examples):
    return tokenizer(examples["text"], padding="max_length", truncation=True, max_length=512)
tokenized_datasets = dataset.map(tokenize_function, batched=True)

#Load and Train the model
model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased',
                                                            num_labels=2,
                                                            pad_token_id=tokenizer.eos_token_id) # Adjust num_labels as needed
model

training_args = TrainingArguments(
    output_dir="./0",
    num_train_epochs=1,
    logging_dir="./logs_bert_custom",
    evaluation_strategy="epoch",
)

# Initialize the Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets['train'],
    eval_dataset=tokenized_datasets['test'],
)

# Start training
trainer.train()

# Define the directory where you want to save your model and tokenizer
model_dir = "./distilbert_finetuned"

# Save the model
model.save_pretrained(model_dir)

# Save the tokenizer
tokenizer.save_pretrained(model_dir)

#Save the model with
trainer.save_model('Distilbert_CusModel_10K')

def make_prediction(text):
  new_complaint=text
  inputs=tokenizer(new_complaint, return_tensors="pt")
  inputs = inputs.to(torch.device("cuda:0"))
  outputs=model(**inputs)
  predictions=outputs.logits.argmax(-1)
  predictions=predictions.detach().cpu().numpy()
  return(predictions)

sample_data["finetuned_predicted"]=sample_data["text"].apply(lambda x: make_prediction(str(x))[0])
sample_data.sample(10)

from sklearn.metrics import confusion_matrix
# Create the confusion matrix
cm1 = confusion_matrix(sample_data["label"], sample_data["finetuned_predicted"])
print(cm1)
accuracy1=cm1.diagonal().sum()/cm1.sum()
print(accuracy1)

"""### Loading a pre-built model and making prediction"""

#Code to donwloading the distilbert model
!gdown --id 1785J3ir19RaZP3ebbFvWUX88PMaBouro -O distilbert_finetuned_V1.zip
!unzip -o -j distilbert_finetuned_V1.zip -d distilbert_finetuned_V1

model_v1 = DistilBertForSequenceClassification.from_pretrained('/content/distilbert_finetuned_V1')
model_v1.to("cuda:0")

def make_prediction(text):
  new_complaint=text
  inputs=tokenizer(new_complaint, return_tensors="pt")
  inputs = inputs.to(torch.device("cuda:0"))
  outputs=model_v1(**inputs)
  predictions=outputs.logits.argmax(-1)
  predictions=predictions.detach().cpu().numpy()
  return(predictions)

sample_data_large=complaints_data.sample(n=1000, random_state=55)
sample_data_large["finetuned_predicted"]=sample_data_large["text"].apply(lambda x: make_prediction(str(x)[:350])[0])

sample_data_large["finetuned_predicted"]

from sklearn.metrics import confusion_matrix
# Create the confusion matrix
cm1 = confusion_matrix(sample_data_large["label"], sample_data_large["finetuned_predicted"])
print(cm1)
accuracy1=cm1.diagonal().sum()/cm1.sum()
print(accuracy1)

# Saving the Model on HuggingFace hub
!pip install transformers
!pip install huggingface_hub
!pip install -U ipykernel #for executing the commands

from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments

!gdown --id 1785J3ir19RaZP3ebbFvWUX88PMaBouro -O distilbert_finetuned_V1.zip
!unzip -o -j distilbert_finetuned_V1.zip -d distilbert_finetuned_V1

model = DistilBertForSequenceClassification.from_pretrained('/content/distilbert_finetuned_V1')

from google.colab import userdata
import os
os.environ['HUGGINGFACEHUB_API_TOKEN']=userdata.get('HUGGINGFACEHUB_API_TOKEN')

from huggingface_hub import notebook_login
notebook_login()
#To get Auth token: Profile >> Settings >>Access Token

model.push_to_hub("Bank_Complaints_distil_bert_10K")

"""# Loading the model from HuggingFace hub"""

model=DistilBertForSequenceClassification.from_pretrained("harshitg1003/Bank_distil_bert_10K")

from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments

tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')

import pandas as pd
!wget https://github.com/venkatareddykonasani/Datasets/raw/master/Bank_Customer_Complaints/complaints_v2.zip
!unzip -o complaints_v2.zip
complaints_data = pd.read_csv("/content/complaints_v2.csv")
list(complaints_data["text"].head())

import torch

complaint="""
payment history missing credit report made mistake put account forbearance without authorization knowledge matter fact automatic payment setup month monthly mortgage paid full noticed issue account marked forbearance credit report tried get new home loan another new bank contacted immediately asked fix error provide letter detail please see asks forbearance issue seemed fixed however credit report payment history missing new bank able approve new loan issue missing payment history contacted time since phone ask thing report payment history transunion fix missing data issue provide letter show account never forbearance payment history past month however waiting week countless email phone call talk multiple supervisor able get either one thing without issue fixed new bank process new loan application therefore need help immediately get fixed
"""

inputs=tokenizer(complaint, return_tensors="pt")
outputs=model(**inputs)
predictions=outputs.logits.argmax(-1)
predictions=predictions.detach().cpu().numpy()
print(predictions)

"""# Web App Creation"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile requirements.txt
# streamlit
# numpy
# pandas
# torch
# transformers
# huggingface_hub

!pip install -r requirements.txt

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# import numpy as np
# import pandas as pd
# import torch
# from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments
# 
# tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')
# model = DistilBertForSequenceClassification.from_pretrained('harshitg1003/Bank_Complaints_distil_bert_10K')
# 
# st.title("Bank Complaints Categorization")
# st.write("Sample Complaints are given below")
# Sample_Complaints = [
#     {"Sentence": "Credit Report - payment history missing credit report made mistake put account forbearance without authorization "},
#     {"Sentence": "Retail Related - forwarded message cc sent friday pdt subject final legal payment well fargo well fargo clearly wrong need look actually opened account see court hearing several different government agency "}
# ]
# st.table(Sample_Complaints)
# user_input = st.text_input("Enter a complaint:")
# button=st.button("Classify")
# 
# d={
#     0: "Credit reporting",
#     1: "Mortgage and Others"
# }
# 
# if user_input and button:
#   inputs=tokenizer(user_input, return_tensors="pt")
#   outputs=model(**inputs)
#   predictions=outputs.logits.argmax(-1)
#   predictions=predictions.detach().cpu().numpy()
#   print(predictions)
#   st.write("Prediction :" , d[predictions[0]])
#

!streamlit run app.py & npx localtunnel --port 8501 & curl ipv4.icanhazip.com

#This sometimes doesn't work on Chrome

